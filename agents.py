from collections import deque

from typing import Optional, Callable
import numpy as np
import torch
import torch.nn as nn
from omegaconf import OmegaConf

import trainer
import utils as U
import dict_operations as O

# %%
'''
agent related functions
'''
def generalized_advantage_estimation(reward, value, end, discount, lambda_gae):
    """
    Compute generalized advantage esimation with the given reward and value (numbers, not function) data

    Follows the transition notation: O_t -- a_t --> O_t+1, r_t+1
    Thus, the first reward is not used to generate the advantage.

    Also, the advantage value is synchronized with the corresponding action: A_t (advantage) -- a_t (action)
    because the advantage results from that action.
    Thus, the last advantage is filled with NaN since there is no action taken after the final observation,
    as well as no reward resulting from that action.

    Parameters
    ----------
    reward : ndarray of shape (n_samples,)
        Note that reward should be synchronized with the observation.
        The first reward is not used
    value : ndarray of shape (n_samples,)
        Note that value should be synchronized with the action
    end : ndarray of shape (n_samples,)
        Note that end should be synchronized with the obseration.
        The end value corresponding to the terminal state should be True.

    Returns
    -------
    advantage : ndarray of shape (n_samples,)
        Generalized advantage which is the discounted sum of TD errors.
        The last advantage is NaN since there is no next reward from the action taken.
    """
    assert len(reward)==len(value)==len(end), f'the lengths of reward ({reward.shape}), value ({value.shape}), end ({end.shape}) must be identical'

    delta = reward[1:] + discount*value[1:] - value[:-1] # shape: (n_samples-1,)
    # delta = np.concatenate((delta, [np.nan]), axis=0) # np.concatenate((delta, np.full(np.nan, delta.shape[1:])), axis=0) also works

    common_ratio = discount*lambda_gae
    advantage = []
    advantage_ = 0
    for delta_, end_ in zip(reversed(delta), reversed(end[:-1])): # Skip the final data sample, since advantage==NaN
        if end_: # action: NaN, observation not correlated with next observation
            advantage.append(np.nan)
            advantage_ = 0
        else:
            advantage_ = delta_ + common_ratio*advantage_ # advantage_t+1 == delta_t+1
            advantage.append(advantage_)
    advantage = np.concatenate((advantage[::-1], [np.nan]), axis=0)
    return advantage

def evaluate_actions(policy, observation_history, action, action_space):
    """
    Might need to apply different equations for different types of action_space

    Parameters
    ----------
    policy : callable, which supports the argument "return_distrib", and returns torch.distributions.Distribution() object
    observation:
    action:
    action_space: gym.spaces.Space() object

    Returns
    -------
    log_action_prob : ndarray of shape (n_classes, n_classes)

        Explanation variable
    """
    action_distribution = policy(observation_history=observation_history, batch=True, return_distrib=True)
    # observation_history_processed = policy.preprocess(observation_history=observation_history)
    # action_distribution = policy.forward(observation_history=observation_history_processed, return_distrib=True)
    device = U.get_device(policy.nn)
    log_action_prob = action_distribution.log_prob(torch.as_tensor(action, device=device)).cpu().numpy()
    log_action_prob = log_action_prob.sum(axis=1) if log_action_prob.ndim > 1 else log_action_prob # for independent actions
    return log_action_prob

def decorator_act(act):
    """
    Apply this @decorator to Agent.act(), so that proper preparation could be completed before acting.
    """
    def prepared_act(self, observation, stochastic=False):
        self.nn.eval()
        observation = self.observation_preprocessor(observation) if self.observation_preprocessor is not None else observation
        action = act(self, observation, stochastic)
        self.workingmemory.append(observation)
        return action
    return prepared_act

def construct_observation_history(observation, timewindow=2, workingmemory=None, observation_history=None):
    """
    When workingmemory is given, observation_history is generated from observation & workingmemory.
    When observation_history is given, observation_history is generated by using observation & observation_history[1:]

    When there aren't enough samples in working memory, the oldest sample is duplicated.

    Parameters
    ----------
    observation: Single observation with shape (observation_dim) or dict of such arrays
    timewindow:
    workingmemory: optional
    observation_history: optional

    Returns
    -------
    observation_history : ndarray of shape (timewindow, observation_dim) or dict of such ndarrays
    """
    dict_observation = isinstance(observation, dict)
    if workingmemory is not None and observation_history is not None:
        raise Exception('only one of workingmemory and observation_history must be provided.')

    elif workingmemory is not None:
        if len(workingmemory)==0: # Copy observation when there's insufficient samples in workingmemory
            observation = O.add_batch_dim(observation) # Add time dimension
            observation_history = O.broadcast_to(observation, shape=(timewindow, *O.shape(observation)[1:]))
        else:
            observation_previous = [observation for observation, i in zip(reversed(workingmemory), range(timewindow-1))] # Extract "timewindow" number of newest observations
            for i in range(timewindow-1-len(observation_previous)): observation_previous.append(observation_previous[-1]) # Copy oldest observation when 1 <= len(workingmemory) < timewindow-1
            observation_previous.reverse()
            observation_previous.append(observation)
            if dict_observation:
                observation_history = {key: np.stack(observation_previous_, axis=0) for key, observation_previous_ in U.merge_dict(observation_previous).items()}
            else:
                observation_history = np.stack(observation_previous)

    elif observation_history is not None:
        observation = O.add_batch_dim(observation) # Add time dimension
        if dict_observation:
            assert all([len(observation_history_)==timewindow for observation_history_ in observation_history.values()]), f'observation_history must have the same length as timewindow ({timewindow}), received: {[len(observation_history_) for observation_history_ in observation_history.values()]}'
            observation_history = {key: np.concatenate((observation_history[key][1:], observation[key]), axis=0) for key in observation.keys()}
        else:
            assert len(observation_history)==timewindow, f'observation_history must have the same length as timewindow ({timewindow}), received: {len(observation_history)}'
            observation_history = np.concatenate((observation_history[1:], observation), axis=0)

    else:
        raise Exception('one of "workingmemory" or "obsevation_history" must be provided')

    return observation_history

def trim_observation_history(observation, end, timewindow, mode='valid'):
    """

    Parameters
    ----------
    observation : array or dict of arrays of shape (n_samples, dim_observation)
        The observation must be ordered in time, since observation_history is parsed from the observation.
    end: boolean array of shape (n_samples,)
    timewindow: int
    mode: one of ['valid', 'all']
        - if 'valid', the observation_history is made by parsing only the valid transitions (referencing the "end" array)
        - if 'all', the observation_history is made for every timepoint, duplicating the most recent observation when the observation is missing.

    Returns
    -------
    observation_histories : ndarray or dict of ndarrays of shape (number_of_valid_transitions or n_samples, timewindow, dim_observation)
    indices: integer ndarray of shape (number_of_valid_transitions or len(observation),)
        indicating the timepoints corresponding to the observation_histories.

    """
    # timewindow=3
    # mode='valid'
    # mode='all'

    mode_list = ['valid', 'all']
    l_observation_history = []
    indices = []

    # Only trim valid transitions
    if mode=='valid':
        for i, (observation_history, end_history) in enumerate(zip(O.nwise(observation, n=timewindow), O.nwise(end, n=timewindow)), timewindow-1):
            if not end_history.any():
                l_observation_history.append(observation_history)
                indices.append(i)
        indices = np.array(indices)

    # Trim all transitions by copying oldest observation in invalid transitions
    elif mode=='all':
        # Copy initial timepoints
        workingmemory = deque(maxlen=timewindow)
        for observation_, end_ in zip(O.__iter__(observation), end):
            observation_history = construct_observation_history(observation_, timewindow=timewindow, workingmemory=workingmemory)
            l_observation_history.append(observation_history)
            workingmemory.append(observation_)
            if end_:
                workingmemory.clear()
        indices = np.arange(len(end))

    else:
        raise Exception(f'Invalid mode: {mode}. Mode should be one of: {mode_list}')

    observation_histories = U.ld_to_array(l_observation_history)
    return observation_histories, indices

def decorator_update(update):
    def prepared_update(self):
        self.prepare_data()
        update(self)
    return prepared_update

# %%
'''
Agent classes
'''
# %%
class BaseAgent(object):
    """
    Without interacting with the environment,
    maintain its own sense of the world (world model) and plan out.
    """
    model_keys = ['policy', 'value'] # When using actorcritic, formulate it into two separate modules
    memory_keys = ['observation', 'reward', 'end', 'info', 'action']
    action_keys = []
    def __init__(self, cfg: dict, nn: nn.Module, trainers: trainer.Trainer, models: dict, observation_preprocessor: Optional[Callable]=None, info: Optional[dict]={}):
        """
        Agent that takes care of everything. This agent just interacts with the environment.

        Parameters
        ----------
        cfg : omegaconf.DictConfig object (or nested Dict-like object)
            cfg should have the following format

            cfg.buffer_size # should be integrated into separate parameters for each model

            cfg.threshold_innate = 5
            cfg.plan = {}
            cfg.plan.max_depth = 4 # Defines the depth of tree search
            cfg.plan.n_options = 3

        nn: nn.ModuleDict object
            with the following keywords ['worldmodel', 'policy', 'value']==SimpleAgent.nn_keys

        trainers: trainer.Trainer object
            Trains the trainer.nn modules using trainer.memory
            PPO, A2C are types of trainers.

        models: dict of callable objects
            Models are used for inference. Receives numpy.ndarray

        observation_preprocessor: Callable
            Initial preprocessor called on all observations.

        info: dict of environment specific variables
            action_space
        """
        ## Consistency check
        assert type(nn)==torch.nn.ModuleDict, f'nn must be torch.nn.ModuleDict type, received: {type(nn)}'
        assert set(self.model_keys) <= set(models.keys()), f'missing nn_keys: {set(self.model_keys) - set(models.keys())}'

        ## Store arguments
        self.cfg = cfg
        self.nn = nn # careful not to reference torch.nn -> torch.nn.Module instances
        self.trainers = trainers
        self.models = models # -> receive numpy arrays and return numpy arrays
        self.observation_preprocessor = observation_preprocessor # observation_preprocessor specific to the envrionment
        self.info = info

        ## Initializations
        self.memory = {memory_name: deque(maxlen=self.cfg.buffer_size+1) for memory_name in self.memory_keys} # Not a pytorch dataset. pytorch dataset is for training purposes only, and should not be a storage
        self.memory['temp'] = None # Temporary variable for memory that are processed into neat numpy arrays

        self.training = True # Might be deprecated, not used for now
        self.timewindow = max([model.timewindow if hasattr(model, 'timewindow') else 0 for model in self.models.values()])
        self.workingmemory = deque(maxlen=self.timewindow-1) # to preprocess observations if needed

        self.probe = U.Counter(keys=self.action_keys)

        # Connecting memory
        for trainer in self.trainers.values():
            trainer.memory = self.memory
        for model in self.models.values():
            model.workingmemory = self.workingmemory

    def __repr__(self):
        return '(models)\n'+'.\n'.join([f'{model_name}: {str(model)}' for model_name, model in self.models.items() if model is not None])

    @decorator_act
    def act(self, observation, stochastic=False):
        pass

    def observe(self, observation, reward, end, info, action):
        """
        Storing data to train each of models within the agent.
        Preprocess the data here if necessary, into a common format that is easy for modules to use for training.

        Parameters
        ----------
        observation : observation received from env.step()
        reward : reward received from env.step()
        end : env received from env.step()
        info : info received from env.step()
        action : action that the agent took
        """
        observation = self.observation_preprocessor(observation) if self.observation_preprocessor is not None else observation
        action = action if action is not None else np.full(self.info['action_space'].shape, np.nan, dtype=np.float32) # When action is None, append nan

        self.memory['observation'].append(observation)
        self.memory['reward'].append(reward)
        self.memory['end'].append(end)
        self.memory['info'].append(info)
        self.memory['action'].append(action)

    def prepare_data(self):
        """
        Preprocesses the data from the memory, and links it to self.memory['temp']
        The subclasses of BaseAgent should overwrite the self.update() with super().update(),
        to ensure the memory['temp'] is properly computed.

        observation: always full of data
        reward: nan at the start of episode (T=0)
        end: always full with data (False at the start of episode (T=0))
        action: nan at the end of episode (T=T)
        value: always full of data (based on observation)
        advantage: nan at the end of episode (advantage defined on transitions), and at terminal observations (terminal observations uncorrelated)
        reward: nan at the end of episode (advantage defined on transitions), and at terminal observations (terminal observations uncorrelated)

        (info: None at the start of episode (T=0))

        Note that on-policy training and off-policy training can be mediated by adjusting the
        - number of epochs within trainers,
        - Buffer size of memory and training interval
        """
        self.nn.train()

        # Extract from memory
        observation = O.astype(U.ld_to_array(self.memory['observation']), np.float32) # Because observation might be a dict of ndarrays
        reward = np.array(self.memory['reward'], dtype=np.float32)
        end = np.array(self.memory['end'], dtype=bool) # Becareful of None, although they will be substitute as None->False
        action = np.array(self.memory['action'], dtype=np.float32)

        # Compute additional values
        # This might compute redundant values if the value_net and policy_net share features/networks. Wrote this for clarity, optimize in the future.
        observation_history, indices = trim_observation_history(observation=observation, end=end, timewindow=self.models['value'].timewindow, mode='all')
        value = self.models['value'](observation_history=observation_history, batch=True).squeeze(1) # Remove batch dimension, dtype float32
        advantage = generalized_advantage_estimation(reward=reward, value=value, end=end, discount=self.cfg.discount, lambda_gae=self.cfg.lambda_gae).astype(np.float32)
        return_ = advantage + value # How to deal with terminal returns? substitute with 0 or NaN?
        log_action_prob = evaluate_actions(policy=self.models['policy'], observation_history=observation_history, action=action, action_space=self.info['action_space'])

        self.memory['temp'] = {'observation': observation, 'observation_history':observation_history, 'reward': reward, 'end': end, 'action': action, 'value': value, 'advantage': advantage, 'return': return_, 'log_action_prob': log_action_prob}

    def update(self):
        pass

    def reset_workingmemory(self):
        self.workingmemory.clear()

    def reset_probe(self):
        if hasattr(self, 'probe'):
            self.probe.reset()

    def reset_memory(self):
        # Reset the memory
        for memory in self.memory.values():
            memory.clear()

    def train(self):
        self.training = True # Same notion as nn.Module().training
    def eval(self):
        self.training = False # Same notion as nn.Module().training

class SimpleAgent(BaseAgent):
    """
    No planning, No innate behavior, No self model
    """
    nn_keys = ['actorcritic']
    model_keys = ['policy', 'value'] # When using actorcritic, formulate it into two separate modules
    action_keys = ['policy_action']

    @decorator_act
    def act(self, observation, stochastic=False):
        self.probe.count('policy_action')
        observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
        action = self.models['policy'](observation_history=observation_history, stochastic=stochastic)
        return action

    @decorator_update
    def update(self):
        self.trainers['actorcritic'].train()

class InnateAgent(SimpleAgent):
    action_keys = ['innate_action', 'policy_action']
    innate_types = ['flight', 'freeze']
    def innate_action(self, observation):
        """
        Evolutionary innate action which helps survival in early stages.

        Parameters
        ----------
        observation : array-like of shape (n_samples,), default=None
            Argument explanation.
            If ``None`` is given, those that appear at least once
            .. versionadded:: 0.18
        Returns
        -------
        innate_action : ndarray of shape (n_classes, n_classes)
        """
        # Freeze & Fight not plausible in current environment
        if self.cfg.innate_type=='freeze':
            innate_action = np.zeros(self.info['action_space'].shape)
        # Flight
        elif self.cfg.innate_type=='flight':
            innate_action = observation['A'] - observation['P'] # Run in the opposite direction
        else:
            raise Exception(f'unknown innate_type: {self.cfg.innate_type}, innate_type must be one of: {self.innate_types}')

        return innate_action

    def _innate_condition(self, observation):
        return np.linalg.norm(observation['A']-observation['P']) < self.cfg.threshold_innate

    @decorator_act
    def act(self, observation, stochastic=False):
        if self._innate_condition(observation):
            self.probe.count('innate_action')
            return self.innate_action(observation)
        else:
            self.probe.count('policy_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            return self.models['policy'](observation_history=observation_history, stochastic=stochastic)

class PlanAgent(SimpleAgent):
    """
    Without interacting with the environment,
    maintain its own sense of the world (world model) and plan out.
    """
    nn_keys = ['worldmodel', 'actorcritic']
    model_keys = ['worldmodel', 'policy', 'value']
    action_keys = ['planned_action', 'policy_action']
    plan_types = ['full', 'sparse']

    def _plan_condition(self, observation):
        '''
        May substitute the plan criterion using the performance (loss) of worldmodel.
        For now, just randomly sample.
        '''
        return np.random.rand() < self.cfg.plan.probability

    @decorator_act
    def act(self, observation, stochastic=False):
        if self._plan_condition(observation):
            self.probe.count('planned_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            observation_history_img = observation_history # Start imagination
            action_img, advantage_img = self.plan(observation_history_img=observation_history_img)  # Plan out
            action = action_img # Execute imagined action in reality
            return action
        else:
            self.probe.count('policy_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            action = self.models['policy'](observation_history=observation_history, stochastic=stochastic) # Act from real observation
            return action

    def plan(self, observation_history_img, depth=0, value_img=0):
        """
        Return the best action from Monte Carlos tree search
        Use advantage-GAE to evaluate actions.
        Perform depth-first-search (DFS) since it's more convenient to compare actions from a single observation_img.

        Parameters
        ----------
        observation_img : Imaginary observation with the same shape as observation.
            May be numpy.ndarray or dict of numpy.ndarray.
        depth: The tree depth of the starting node

        Returns
        -------
        action_img : ndarray of shape (action_dim)
        """
        depth = depth + 1
        action_img_list, advantage_img_list = [], []

        if self.cfg.plan.type=='full':
            n_options=self.cfg.plan.n_options
        elif self.cfg.plan.type == 'sparse':
            if depth==1:
                n_options=self.cfg.plan.n_options
            else:
                n_options=1
        else:
            raise Exception(f'unknown plan.type: {self.cfg.plan.type}, innate_type must be one of: {self.plan_types}')

        for i in range(n_options): # OPTION: May parallize here using pool
            # Simulate future
            observation_history_img_actorcritic = O.__getitem__(observation_history_img, slice(-self.models['policy'].timewindow, None)) # [-timewindow:]
            action_img = self._imaginary_policy(observation_history_img_actorcritic, stochastic=(i!=0))

            observation_history_img_worldmodel = O.__getitem__(observation_history_img, slice(-self.models['worldmodel'].timewindow, None)) # [-timewindow:]
            observation_img_next, reward_img_next = self.models['worldmodel'](observation_history=observation_history_img_worldmodel, action=action_img)

            # Evaluate action using advantage-GAE
            observation_history_img_actorcritic_next = construct_observation_history(observation=observation_img_next, timewindow=self.models['value'].timewindow, observation_history=observation_history_img_actorcritic)
            value_img_next = self._imaginary_value(observation_history_img=observation_history_img_actorcritic_next)
            delta_img = reward_img_next + self.cfg.discount*value_img_next - value_img

            if depth==self.cfg.plan.max_depth:
                advantage_img_next = 0
            else:
                # action_img_next is not used. Maybe it could be queued in the future.
                observation_history_img_next = construct_observation_history(observation=observation_img_next, timewindow=self.timewindow, observation_history=observation_history_img)
                action_img_next, advantage_img_next = self.plan(observation_history_img=observation_history_img_next, depth=depth, value_img=value_img_next)

            # Compute advantage
            # TODO: Could have a separate lambda_gae for imagination, but use the lambda_gae used in training for now.
            advantage_img = delta_img + self.cfg.discount*self.cfg.lambda_gae*advantage_img_next

            action_img_list.append(action_img)
            advantage_img_list.append(advantage_img)

        best_action_i = np.argmax(advantage_img_list)
        action_img_best, advantage_img_best = action_img_list[best_action_i], advantage_img_list[best_action_i]
        return action_img_best, advantage_img_best

    def _imaginary_policy(self, observation_history_img, stochastic=False):
        action_img = self.models['policy'](observation_history=observation_history_img, stochastic=stochastic) # One children node is always the mean action
        return action_img

    def _imaginary_value(self, observation_history_img):
        value_img = self.models['value'](observation_history=observation_history_img)
        return value_img

    @decorator_update
    def update(self):
        self.trainers['actorcritic'].train()
        self.trainers['worldmodel'].train()

class PlanSelfAgent(PlanAgent):
    nn_keys = ['worldmodel', 'selfmodel', 'actorcritic']
    model_keys = ['worldmodel', 'selfmodel', 'policy', 'value']

    def _imaginary_policy(self, observation_history_img, stochastic=False):
        action_img, value_img = self.models['selfmodel'](observation_history=observation_history_img, stochastic=stochastic) # One children node is always the mean action
        return action_img

    def _imaginary_value(self, observation_history_img):
        action_img, value_img = self.models['selfmodel'](observation_history=observation_history_img, stochastic=False) # One children node is always the mean action
        return value_img

    @decorator_update
    def update(self):
        self.trainers['actorcritic'].train()
        self.trainers['worldmodel'].train()
        self.prepare_data()
        self.trainers['selfmodel'].train()

class PlanSelfPolicyAgent(PlanSelfAgent):
    '''
    Agent with selfmodel policy + actorcritic value
    '''
    def _imaginary_value(self, observation_history_img):
        value_img = self.models['value'](observation_history=observation_history_img)
        return value_img

class PlanSelfValueAgent(PlanSelfAgent):
    '''
    Agent with selfmodel policy + actorcritic value
    '''
    def _imaginary_policy(self, observation_history_img, stochastic=False):
        action_img = self.models['policy'](observation_history=observation_history_img, stochastic=stochastic) # One children node is always the mean action
        return action_img

class PlanInnateAgent(PlanAgent, InnateAgent):
    action_keys = ['innate_action', 'planned_action', 'policy_action']

    @decorator_act
    def act(self, observation, stochastic=False):
        if self._innate_condition(observation):
            self.probe.count('innate_action')
            return self.innate_action(observation)
        elif self._plan_condition(observation):
            observation_history = construct_observation_history(observation=observation, timewindow=self.timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            observation_history_img = observation_history # Start imagination
            action_img, advantage_img = self.plan(observation_history_img=observation_history_img)  # Plan out
            self.probe.count('planned_action')
            action = action_img # Execute imagined action in reality
            return action
        else:
            self.probe.count('policy_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            action = self.models['policy'](observation_history=observation_history, stochastic=stochastic) # Act from real observation
            return action

class PlanInnateTriggerAgent(PlanAgent, InnateAgent):
    action_keys = ['innate_action', 'planned_action', 'aborted_planned_action', 'policy_action']

    @decorator_act
    def act(self, observation, stochastic=False):
        if self._innate_condition(observation):
            self.probe.count('innate_action')
            return self.innate_action(observation)
        elif self._plan_condition(observation):
            observation_history = construct_observation_history(observation=observation, timewindow=self.timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            observation_history_img = observation_history # Start imagination
            action_img, advantage_img, innate_triggered = self.plan(observation_history_img=observation_history_img)  # Plan out
            if innate_triggered:
                self.probe.count('aborted_planned_action')
            else:
                self.probe.count('planned_action')
            action = action_img # Execute imagined action in reality
            return action
        else:
            self.probe.count('policy_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            action = self.models['policy'](observation_history=observation_history, stochastic=stochastic) # Act from real observation
            return action

    def plan(self, observation_history_img, depth=0, value_img=0):
        """
        Return the best action from Monte Carlos tree search
        Use advantage-GAE to evaluate actions.
        Perform depth-first-search (DFS) since it's more convenient to compare actions from a single observation_img.

        Parameters
        ----------
        observation_img : Imaginary observation with the same shape as observation.
            May be numpy.ndarray or dict of numpy.ndarray.
        depth: The tree depth of the starting node

        Returns
        -------
        action_img : ndarray of shape (action_dim)
        """
        depth = depth + 1
        action_img_list, advantage_img_list = [], []

        if self.cfg.plan.type=='full':
            n_options=self.cfg.plan.n_options
        elif self.cfg.plan.type == 'sparse':
            if depth==1:
                n_options=self.cfg.plan.n_options
            else:
                n_options=1
        else:
            raise Exception(f'unknown plan.type: {self.cfg.plan.type}, innate_type must be one of: {self.plan_types}')

        for i in range(n_options): # OPTION: May parallize here using pool
            # Simulate future
            observation_history_img_actorcritic = O.__getitem__(observation_history_img, slice(-self.models['policy'].timewindow, None)) # [-timewindow:]
            action_img = self.models['policy'](observation_history=observation_history_img_actorcritic, stochastic=(i!=0)) # One children node is always the mean action

            observation_history_img_worldmodel = O.__getitem__(observation_history_img, slice(-self.models['worldmodel'].timewindow, None)) # [-timewindow:]
            observation_img_next, reward_img_next = self.models['worldmodel'](observation_history=observation_history_img_worldmodel, action=action_img)

            # Check innate_condition on imaginary observation
            if self._innate_condition(observation_img_next):
                return self.innate_action(observation_img_next), None, True

            # Evaluate action using advantage-GAE
            observation_history_img_actorcritic_next = construct_observation_history(observation=observation_img_next, timewindow=self.models['value'].timewindow, observation_history=observation_history_img_actorcritic)
            value_img_next = self.models['value'](observation_history=observation_history_img_actorcritic_next)
            delta_img = reward_img_next + self.cfg.discount*value_img_next - value_img

            if depth==self.cfg.plan.max_depth:
                advantage_img_next = 0
            else:
                # action_img_next is not used. Maybe it could be queued in the future.
                observation_history_img_next = construct_observation_history(observation=observation_img_next, timewindow=self.timewindow, observation_history=observation_history_img)
                action_img_next, advantage_img_next, innate_triggered = self.plan(observation_history_img=observation_history_img_next, depth=depth, value_img=value_img_next)

                if innate_triggered: # Walk up the tree (Abort planning)
                    return action_img_next, advantage_img_next, innate_triggered

            # Compute advantage
            # TODO: Could have a separate lambda_gae for imagination, but use the lambda_gae used in training for now.
            advantage_img = delta_img + self.cfg.discount*self.cfg.lambda_gae*advantage_img_next

            action_img_list.append(action_img)
            advantage_img_list.append(advantage_img)

        best_action_i = np.argmax(advantage_img_list)
        action_img_best, advantage_img_best = action_img_list[best_action_i], advantage_img_list[best_action_i]
        return action_img_best, advantage_img_best, False

class PlanInnateSelfAgent(PlanSelfAgent, InnateAgent):
    nn_keys = ['worldmodel', 'selfmodel', 'actorcritic']
    model_keys = ['worldmodel', 'selfmodel', 'policy', 'value']
    action_keys = ['innate_action', 'planned_action', 'policy_action']

    @decorator_act
    def act(self, observation, stochastic=False):
        if self._innate_condition(observation):
            self.probe.count('innate_action')
            return self.innate_action(observation)
        elif self._plan_condition(observation):
            self.probe.count('planned_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            observation_history_img = observation_history # Start imagination
            action_img, advantage_img = self.plan(observation_history_img=observation_history_img)  # Plan out
            action = action_img # Execute imagined action in reality
            return action
        else:
            self.probe.count('policy_action')
            observation_history = construct_observation_history(observation=observation, timewindow=self.models['policy'].timewindow, workingmemory=self.workingmemory) # dict of shape (time, observation_dim)
            action = self.models['policy'](observation_history=observation_history, stochastic=stochastic) # Act from real observation
            return action
